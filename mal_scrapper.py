#!/usr.bin/env python3
##
# Dakara Project
#

import bs4 as bs, json, html, os, re, requests

json_file_path = ''

def get_artists(tags, force_scrap=False):
    """
    Find and save the artists of an anime theme by scrapping MyAnimeList.
    The scrapped data will be cached to reduce later parsing times.

    tags is a dictionnary generated by anime_parse.parse_file_name (will be edited with the found artists)
    """

    # We don't care about IS
    if tags['link_type'] not in ['OP', 'ED']:
        return []

    global json_file_path
    json_file_path = os.path.dirname(os.path.realpath(__file__)) + '/mal_scrapper.json'

    # We first load the local data
    json_file = open(json_file_path, 'r')
    saved_data = json.load(json_file)
    json_file.close()

    name = tags['title_work']
    if tags['subtitle_work'] != '':
        name = name + ' ~ ' + tags['subtitle_work']

    # If we don't find the anime, let's scrap MAL
    scrapped = False
    if name not in saved_data[0] or force_scrap: # We check if we don't already have the anime scrapped
        scrap_anime(name, saved_data)
        scrapped = True

    # If we find the anime, let's get the artists
    if name in saved_data[0]:
        index = str(tags['link_nb'])
        if index == '0': # If the index either 0, it will still be stored as 1
            index = '1'

        if index in saved_data[0][name][tags['link_type']]: # Index in list, we use it
            return saved_data[0][name][tags['link_type']][index]
        elif not scrapped: # Index not in list but no scrapping done... Local data may not be updated so we do it now
            return get_artists(tags, True)
        else: # Index not in list despite scrapping... Can't do anything else here
            return []
    else:
        return []


def scrap_anime(name, data):
    """
    This function does the actual scrapping and saves it in data.
    """

    # First steap: finding the page of the anime, using the search engine
    r = requests.get('https://myanimelist.net/anime.php?q=' + name)
    found = {}

    i = 0
    N = 10 # Max number of results kept

    parsed = bs.BeautifulSoup(r.text, features='html.parser')
    animes_list = parsed.find('div', {'class': 'js-block-list'}).table.findAll('tr')

    for i in range(1, N + 1): # We won't keep more than N entries
        anime = html.unescape(animes_list[i].find('strong').string)
        link = animes_list[i].find('a', {'class': 'hoverinfo_trigger'})['href']
        found[anime] = link # We store the result and continue

        if anime == name: # If we find a perfect match, that's great!
            break
    
    if anime != name: # If we didn't get a perfect match, we will ask the user
        animes_found = list(found)
        print()
        for i in range(len(animes_found)):
            print(str(i) + ": " + animes_found[i])
        match_index = int(input(name + ": which is the right one? "))
        if (match_index >= N): # No match, no need to continue
            return

        anime = animes_found[match_index]

    # Second step: getting the page of the found anime
    r = requests.get(found[anime])
    parsed = bs.BeautifulSoup(r.text, features='html.parser')

    # Third step: extracting the artists
    op = extract_artists(parsed.find('div', {'class': 'opnening'}))
    ed = extract_artists(parsed.find('div', {'class': 'ending'}))

    # Fourth step: saving the data locally
    data[0][name] = {}
    data[0][name]['OP'] = op
    data[0][name]['ED'] = ed

    json_file = open(json_file_path, 'w')
    json.dump(data, json_file)
    json_file.close()


def extract_artists(tag):
    """
    When scrapping, extracts the artists from the div that contains them.
    """
    artists = {}
    for entry in tag.findAll('span', {'class': 'theme-song'}): # For each theme
        temp_artists = []
        number = re.sub('^#([0-9]+).*$', r'\1', entry.string) # This gets the id of the theme
        if number == entry.string:
            number = '1'
        found = re.sub('^.*[\"\)]:? *by (.*)$', r'\1', html.unescape(entry.string)) # This gets the artists (by finding 'by')
        if found == html.unescape(entry.string):
            found = re.sub('^.*[\"\)]:? *composed by (.*)$', r'\1', html.unescape(entry.string))
            if found == html.unescape(entry.string):
                found = re.sub('^.*[\"\)]:? *- (.*)$', r'\1', html.unescape(entry.string))
                if found == html.unescape(entry.string):
                    found = ''
        found = re.sub(' \(.*ep[^\(]*\)$', '', found) # This removes the information about the episodes (by finding 'ep')
        found = found.encode("ascii", errors="ignore").decode() # This removes the non ASCII characters
        found = re.sub(' \( *\)', '', found) # This removes the parenthesis with spaces only (after non ASCII deletion)
        for artist in found.split(', '): # Here we split if there are multiple artists
            artist = re.sub('^.* \((.*)\)', r'\1', artist) # For the cases where the real artist is in parenthesis
            temp_artists.append(artist)
        artists[number] = temp_artists

    return artists
